{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "transcripts = pd.read_csv(\"transcripts_topic.tsv\", sep=\"\\t\")\n",
    "scores_train = pd.read_csv(\"train_split_Depression_AVEC2017.csv\")\n",
    "scores_dev = pd.read_csv(\"dev_split_Depression_AVEC2017.csv\")\n",
    "scores_test = pd.read_csv(\"test_split_Depression_AVEC2017.csv\")\n",
    "scores = pd.concat([scores_train, scores_dev, scores_test])\n",
    "scores = scores.set_index(\"Participant_ID\")\n",
    "scores = scores[\"PHQ8_Binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>speaker</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_value</th>\n",
       "      <th>sub_topic</th>\n",
       "      <th>participant</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173.236</td>\n",
       "      <td>174.446</td>\n",
       "      <td>Participant</td>\n",
       "      <td>what do you mean i'm sorry</td>\n",
       "      <td>4.0</td>\n",
       "      <td>do you consider yourself an introvert</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>756.786</td>\n",
       "      <td>757.876</td>\n",
       "      <td>Participant</td>\n",
       "      <td>oh wow</td>\n",
       "      <td>1.0</td>\n",
       "      <td>how easy is it for you to get a good night's s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>759.366</td>\n",
       "      <td>761.846</td>\n",
       "      <td>Participant</td>\n",
       "      <td>i have my days um</td>\n",
       "      <td>1.0</td>\n",
       "      <td>how easy is it for you to get a good night's s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>816.806</td>\n",
       "      <td>821.326</td>\n",
       "      <td>Participant</td>\n",
       "      <td>what am i like irritated tired um lazy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>what are you like when you don't sleep well</td>\n",
       "      <td>1.0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>822.486</td>\n",
       "      <td>823.416</td>\n",
       "      <td>Participant</td>\n",
       "      <td>you know</td>\n",
       "      <td>1.0</td>\n",
       "      <td>what are you like when you don't sleep well</td>\n",
       "      <td>1.0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_time  stop_time      speaker                                   value  \\\n",
       "0     173.236    174.446  Participant              what do you mean i'm sorry   \n",
       "1     756.786    757.876  Participant                                  oh wow   \n",
       "2     759.366    761.846  Participant                       i have my days um   \n",
       "3     816.806    821.326  Participant  what am i like irritated tired um lazy   \n",
       "4     822.486    823.416  Participant                                you know   \n",
       "\n",
       "   topic                                        topic_value  sub_topic  \\\n",
       "0    4.0              do you consider yourself an introvert        0.0   \n",
       "1    1.0  how easy is it for you to get a good night's s...        0.0   \n",
       "2    1.0  how easy is it for you to get a good night's s...        0.0   \n",
       "3    1.0        what are you like when you don't sleep well        1.0   \n",
       "4    1.0        what are you like when you don't sleep well        1.0   \n",
       "\n",
       "   participant  Gender  \n",
       "0          303       0  \n",
       "1          303       0  \n",
       "2          303       0  \n",
       "3          303       0  \n",
       "4          303       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Participant_ID\n",
       "303    0.0\n",
       "304    0.0\n",
       "305    0.0\n",
       "310    0.0\n",
       "312    0.0\n",
       "Name: PHQ8_Binary, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual dictionary for topic/subtopic special tokens for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_subtopic_to_category = {\n",
    "    0 : {\n",
    "        0 : \"did_recently\",\n",
    "        1 : \"enjoy_travelling\",\n",
    "        3 : \"family_relationship\",\n",
    "        4 : \"do_for_fun\",\n",
    "        5 : \"best_friend\",\n",
    "        6 : \"ideal_weekend\"\n",
    "    },\n",
    "    \n",
    "    1 : {\n",
    "        0 : \"easy_sleep\",\n",
    "        1 : \"sleep_badly\"\n",
    "    },\n",
    "    \n",
    "    2 : {\n",
    "        0 : \"happy_last_time\",\n",
    "        1 : \"behaviour_changes\",\n",
    "        2 : \"disturbing_thoughts\",\n",
    "        3 : \"feel_lately\"\n",
    "    },\n",
    "    \n",
    "    3 : {\n",
    "        0 : \"any_regret\",\n",
    "        1 : \"feel_guilty\",\n",
    "        2 : \"most_proud\",\n",
    "    },\n",
    "    \n",
    "    4 : {\n",
    "        0 : \"introvert\",\n",
    "        1 : \"shy_outgoing\",\n",
    "    },\n",
    "    \n",
    "    5 : {\n",
    "        0 : \"ptsd_diagnosed\",\n",
    "        1 : \"depression_diagnosed\",\n",
    "        2 : \"therapy_useful\",\n",
    "        \n",
    "    },\n",
    "    \n",
    "    6 : {\n",
    "        2 : \"easy_parent\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pariticipant ID to text dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO can add preprocessing steps here\n",
    "def preprocess(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_to_text = {}\n",
    "\n",
    "prev_topic = \"\"\n",
    "prev_subtopic = \"\"\n",
    "prev_participant = -1\n",
    "\n",
    "for idx, row in transcripts.iterrows():\n",
    "    participant = row.participant\n",
    "    topic = int(row.topic)\n",
    "    subtopic = int(row.sub_topic)\n",
    "    text = row.value\n",
    "    \n",
    "    if participant not in participant_to_text:\n",
    "        # Create blank entry for new participant\n",
    "        participant_to_text[participant] = [\"\", []]\n",
    "    \n",
    "    if participant == prev_participant and topic == prev_topic and subtopic == prev_subtopic:\n",
    "        # If previous participant and topic+subtopic, don't pad special tokens\n",
    "        proc_text = preprocess(text) + \" \"\n",
    "        participant_to_text[participant][0] += proc_text\n",
    "        participant_to_text[participant][1][-1] += proc_text\n",
    "    else:\n",
    "        # If different topic+subtopic, pad special token infront of text before appending to full text and topic-wise text\n",
    "        proc_text = topic_to_subtopic_to_category[topic][subtopic] + \" \" + preprocess(text) + \" \"\n",
    "        participant_to_text[participant][0] += proc_text\n",
    "        participant_to_text[participant][1].append(proc_text)\n",
    "        \n",
    "    prev_participant = participant\n",
    "    prev_topic = topic\n",
    "    prev_subtopic = subtopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text (all topics): \n",
      "introvert what do you mean i'm sorry easy_sleep oh wow i have my days um sleep_badly what am i like irritated tired um lazy you know depression_diagnosed no best_friend i don't really have a best friend but a person that i deal with and i used to work with um she would probably tell you that i'm very um outgoing a go getter dependable responsible happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean \n",
      "\n",
      "Topic-wise text: \n",
      "[\"introvert what do you mean i'm sorry \", 'easy_sleep oh wow i have my days um ', 'sleep_badly what am i like irritated tired um lazy you know ', 'depression_diagnosed no ', \"best_friend i don't really have a best friend but a person that i deal with and i used to work with um she would probably tell you that i'm very um outgoing a go getter dependable responsible \", \"happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean \"]\n"
     ]
    }
   ],
   "source": [
    "# Example output\n",
    "print(\"Full text (all topics): \")\n",
    "print(participant_to_text[303][0])\n",
    "print(\"\\nTopic-wise text: \")\n",
    "print(participant_to_text[303][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test data\n",
    "- Currently, use dev set data as Test data and training set data as Training data\n",
    "- **IMPORTANT**: For training data, we are doing data augmentation as follows:\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation parameters \n",
    "# Here, 0-> non-depressed class, 1-> depressed class\n",
    "# We treat the two classes differently (we do more augmentation for depressed class)\n",
    "min_len = {0: 10, 1: 5}  # Minimum length of a transcript above which we can do augmentation \n",
    "aug_count = {0: 3, 1: 8} # Number of augmented transcripts to be created\n",
    "\n",
    "data_train = {\"Text\": [], \"Targets\": []}\n",
    "data_test = {\"Text\": [], \"Targets\": []}\n",
    "\n",
    "for participant in participant_to_text:\n",
    "    # Training data\n",
    "    if participant in scores_train[\"Participant_ID\"].values:\n",
    "        # Add un-augmented transcript\n",
    "        data_train[\"Text\"].append(participant_to_text[participant][0])\n",
    "        data_train[\"Targets\"].append(scores[participant])\n",
    "        \n",
    "        # Data augmentation step (only for those transcripts which are longer than min_len)\n",
    "        if len(participant_to_text[participant][1]) > min_len[scores[participant]]:\n",
    "            # Generate aug_count integers, each of which is the length of the new transcript \n",
    "            # (each entry in t_len is in range min_len to size of current transcript)\n",
    "            t_lens = np.random.randint(low=min_len[scores[participant]], \n",
    "                                       high=len(participant_to_text[participant][1]), \n",
    "                                       size=aug_count[scores[participant]])\n",
    "            for t_len in t_lens:\n",
    "                # Generate list of all combinations of topic texts of t_len\n",
    "                combs = list(combinations(participant_to_text[participant][1], t_len))\n",
    "                # Select a random combination\n",
    "                t_comb = list(combs[np.random.randint(len(combs))])\n",
    "                # Shuffle the topic texts in selected combination\n",
    "                np.random.shuffle(t_comb)\n",
    "                # Add augmented transcript\n",
    "                data_train[\"Text\"].append(\" \".join(t_comb))\n",
    "                data_train[\"Targets\"].append(scores[participant])\n",
    "\n",
    "    # Testing data\n",
    "    elif participant in scores_dev[\"Participant_ID\"].values:\n",
    "        data_test[\"Text\"].append(participant_to_text[participant][0])\n",
    "        data_test[\"Targets\"].append(scores[participant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([272, 262]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class balance in training data\n",
    "np.unique(data_train[\"Targets\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"introvert what do you mean i'm sorry easy_sleep oh wow i have my days um sleep_badly what am i like irritated tired um lazy you know depression_diagnosed no best_friend i don't really have a best friend but a person that i deal with and i used to work with um she would probably tell you that i'm very um outgoing a go getter dependable responsible happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean \",\n",
       " \"family_relationship very close even though i don't live with them i try to see them as much as i can introvert mm yes  enjoy_travelling um trying new things seeing new views of the world um trying the different type of foods um seeing how the government is and how they run the things out there i guess easy_sleep it's pretty good eh somewhat sleep_badly i'm tired <laughter> and i kind of fall asleep during class and whatnot depression_diagnosed no best_friend very friendly and funny talkative happy_last_time um last weekend i guess \",\n",
       " \"do_for_fun uh fun i like going to the beach uh family_relationship not uh i don't have much family as it is easy_sleep um it's been hard lately it's been probably hard for the last uh going on a year um sleep_badly um tired <laughter> therapy_useful pardon me i still didn't hear you that people have been deceitful depression_diagnosed uh i've been diagnosed with uh bipolarism best_friend how does my friends describe me \",\n",
       " \"enjoy_travelling uh i like traveling by train it's not my favorite thing introvert oh yeah sure absolutely yeah  family_relationship not at all depression_diagnosed <clears throat> no behaviour_changes i'm sorry you repeat that no not necessarily <clears throat> easy_sleep very i'm a heavy sleeper happy_last_time <sigh> uh most_proud most proud of did_recently uh spent new year's eve with a friend real close friend so any_regret in terms of what best_friend oh i don't have a best friend these days <laughter> \",\n",
       " \"depression_diagnosed <clears throat> no  most_proud most proud of  introvert oh yeah sure absolutely yeah   any_regret in terms of what  behaviour_changes i'm sorry you repeat that no not necessarily <clears throat>  easy_sleep very i'm a heavy sleeper  enjoy_travelling uh i like traveling by train it's not my favorite thing  did_recently uh spent new year's eve with a friend real close friend so  best_friend oh i don't have a best friend these days <laughter>  happy_last_time <sigh> uh \",\n",
       " \"did_recently uh spent new year's eve with a friend real close friend so  best_friend oh i don't have a best friend these days <laughter>  easy_sleep very i'm a heavy sleeper  depression_diagnosed <clears throat> no  introvert oh yeah sure absolutely yeah   behaviour_changes i'm sorry you repeat that no not necessarily <clears throat>  family_relationship not at all  happy_last_time <sigh> uh  enjoy_travelling uh i like traveling by train it's not my favorite thing  most_proud most proud of \",\n",
       " \"happy_last_time <sigh> uh  most_proud most proud of  depression_diagnosed <clears throat> no  easy_sleep very i'm a heavy sleeper  best_friend oh i don't have a best friend these days <laughter>  enjoy_travelling uh i like traveling by train it's not my favorite thing  did_recently uh spent new year's eve with a friend real close friend so  introvert oh yeah sure absolutely yeah   family_relationship not at all  any_regret in terms of what \",\n",
       " \"enjoy_travelling i don't i don't enjoy traveling family_relationship fairly close um i see them pretty  introvert yes best_friend i don't really have a best friend introverted remember most_proud right now it's the fact that happy_last_time um a few weeks ago when i got a good grade in a class depression_diagnosed nope easy_sleep it depends if i workout really hard the night before <bef> the day the day before if workout pretty hard it's bad but if i don't workout it i don't know just eh it's hard sleep_badly i'm probably like i am now normal i think not as not as happy about everything  but still pretty okay i can still function behaviour_changes no maybe slightly any_regret no not really \",\n",
       " \"sleep_badly i'm probably like i am now normal i think not as not as happy about everything  but still pretty okay i can still function  behaviour_changes no maybe slightly  happy_last_time um a few weeks ago when i got a good grade in a class  most_proud right now it's the fact that  any_regret no not really  enjoy_travelling i don't i don't enjoy traveling  best_friend i don't really have a best friend introverted remember  family_relationship fairly close um i see them pretty   easy_sleep it depends if i workout really hard the night before <bef> the day the day before if workout pretty hard it's bad but if i don't workout it i don't know just eh it's hard  depression_diagnosed nope \",\n",
       " \"depression_diagnosed nope  any_regret no not really  family_relationship fairly close um i see them pretty   happy_last_time um a few weeks ago when i got a good grade in a class  best_friend i don't really have a best friend introverted remember  enjoy_travelling i don't i don't enjoy traveling  behaviour_changes no maybe slightly  introvert yes  sleep_badly i'm probably like i am now normal i think not as not as happy about everything  but still pretty okay i can still function  most_proud right now it's the fact that \"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"Text\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_train).to_csv(\"data_train.csv\", index=False)\n",
    "pd.DataFrame(data_test).to_csv(\"data_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many words are there in training and testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1720, 823)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = \" \".join(data_train[\"Text\"])\n",
    "train_text = np.unique(train_text.split(\" \"))\n",
    "test_text = \" \".join(data_test[\"Text\"])\n",
    "test_text = np.unique(test_text.split(\" \"))\n",
    "len(train_text), len(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many words in testing data are not found in training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for w in test_text:\n",
    "    if w not in train_text:\n",
    "        x.append(w)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
